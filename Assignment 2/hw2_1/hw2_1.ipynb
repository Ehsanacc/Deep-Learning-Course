{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Feel free to import any other libraries and modules.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "i7I7O1kv0S21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part One: Optimization Algorithms\n",
        "## 1. Gradient Descent\n",
        "The Gradient Descent (GD) algorithm finds the minimum of a given\n",
        "function by taking small steps along the function's gradient:\n",
        "\n",
        ">$\\Theta \\leftarrow \\Theta_0$\n",
        "\n",
        ">**while** stop condition not met **do**\n",
        "\n",
        ">$~~~~$$\\Theta \\leftarrow \\Theta - \\alpha \\nabla_\\Theta f(\\Theta)$\n",
        "\n",
        ">**end while**\n",
        "\n",
        "where $f$ is the function to minimize, $\\nabla_\\Theta f(\\Theta)$\n",
        "denotes $f$'s gradient at $\\Theta$ and $\\alpha$ is the learning rate.\n",
        "\n",
        "**Task1:** Implement the GD algorithm as a function:\n",
        "\n",
        "  \\begin{equation}\n",
        "      \\Theta_{opt} = \\text{GD}(f, \\Theta_0, \\alpha, \\rho)\n",
        "  \\end{equation}\n",
        "where $f$ is a function returning the cost and its gradient with respect to parameter vector $\\Theta$, $\\Theta_0$ is the initial value, and $\\alpha$\n",
        "is the learning rate. You can assume that $\\alpha$. remains constant during the optimization. $\\rho$ is stop condition. \\\\\n",
        "Then, use the GD algorithm to find the optimum of the [Rosenbrock function](https://en.wikipedia.org/wiki/Rosenbrock_function) (Consider $a=1, b=100$).\n",
        " \\\\\n",
        "Also, plot the values found by GD at subsequent iterations.\n",
        "\n",
        "## 2. Newton's Method\n",
        "Newton's method is an iterative optimization algorithm used to find the minimum of a function.\n",
        "The basic update step in Newton's method is given by:\n",
        "\n",
        "\\begin{equation}\n",
        "\\Theta = \\Theta - H^{-1} \\cdot \\nabla f(\\Theta)\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "Where  $ H $ is the Hessian matrix of the function at $ \\Theta $, and $ H^{-1} $ is the inverse of the Hessian matrix, used to adjust the step size and direction more accurately than just using the gradient alone (as done in gradient descent).\n",
        "\n",
        "#### Line Search\n",
        "Sometimes, Newton's method may take too large of a step, which can lead to divergence. To prevent this, a simple **line search** is used. This reduces the step size $ \\alpha $ if the function value doesn't improve after the step.\n",
        "\n",
        "Steps:\n",
        "1. Compute the gradient and Hessian matrix at the current point.\n",
        "2. Calculate the step direction by multiplying the inverse of the Hessian with the gradient.\n",
        "3. Update the point by subtracting the step from the current point.\n",
        "4. If the function value doesn't improve, reduce the step size $ \\alpha $.\n",
        "5. Repeat until the gradient becomes sufficiently small (close to zero), indicating convergence.\n",
        "\n",
        "\n",
        "\n",
        "**Task2:** Implement Newton's method and compare it with the gradient descent. You will also need to implement a line search alogithm, e.g. (https://en.wikipedia.org/wiki/Backtracking_line_search) and make sure that the Newton's direction is indeed one along which the function is minimized.\n"
      ],
      "metadata": {
        "id": "CeXRppkryQBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rosenbrock(x):\n",
        "    \"\"\"Returns the value and gradient of Rosenbrock's function at x: 2d vector\"\"\"\n",
        "    val = #TODO\n",
        "    dv_dx0 = #TODO\n",
        "    dv_dx1 = #TODO\n",
        "    grad = np.array([dv_dx0, dv_dx1])\n",
        "\n",
        "    return val, grad\n",
        "\n",
        "def rosenbrock_hessian(x):\n",
        "    \"\"\"Returns the value, gradient and hessian of Rosenbrock's function at x: 2d vector\"\"\"\n",
        "    val, grad = rosenbrock(x)\n",
        "\n",
        "    # Hessian matrix components\n",
        "    d2v_dx0x0 = #TODO\n",
        "    d2v_dx0x1 = #TODO\n",
        "    d2v_dx1x0 = #TODO\n",
        "    d2v_dx1x1 = #TODO\n",
        "\n",
        "    # Hessian matrix\n",
        "    hessian = np.array([[d2v_dx0x0, d2v_dx0x1],\n",
        "                [d2v_dx1x0, d2v_dx1x1]])\n",
        "\n",
        "    return val, grad, hessian\n",
        "\n",
        "\n",
        "def GD(f, theta0, alpha, stop_tolerance=1e-10, max_steps=1000000):\n",
        "    \"\"\"Runs gradient descent algorithm on f.\n",
        "\n",
        "    Args:\n",
        "        f: function that when evaluated on a Theta of same dtype and shape as Theta0\n",
        "            returns a tuple (value, dv_dtheta) with dValuedTheta of the same shape\n",
        "            as Theta\n",
        "        theta0: starting point\n",
        "        alpha: step length\n",
        "        stop_tolerance: stop iterations when improvement is below this threshold\n",
        "        max_steps: maximum number of steps\n",
        "    Returns:\n",
        "        tuple:\n",
        "        - theta: optimum theta found by the algorithm\n",
        "        - history: list of length num_steps containing tuples (theta, (val, dv_dtheta: np.array))\n",
        "\n",
        "    \"\"\"\n",
        "    history = []\n",
        "\n",
        "    theta = theta0\n",
        "\n",
        "    step = 0\n",
        "    while step < max_steps:\n",
        "      #TODO\n",
        "\n",
        "    history.append([theta, f(theta)])\n",
        "    return theta, history"
      ],
      "metadata": {
        "id": "DIsMJPFs-kLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the optimum of rosenbrock function\n",
        "\n",
        "X0 = [0.,2.]\n",
        "Xopt, Xhist = GD(rosenbrock, X0, alpha=1e-3, stop_tolerance=1e-10, max_steps=1e6)\n",
        "\n",
        "print (\"Found optimum at %s in %d steps (true minimum is at [1,1])\" % (Xopt, len(Xhist)))\n",
        "\n",
        "# Plot how the value changes over iterations\n",
        "#TODO\n"
      ],
      "metadata": {
        "id": "yyb5unZNaSFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Newton's Method\n",
        "def Newton(f, theta0, alpha=1, stop_tolerance=1e-10, max_steps=1000000):\n",
        "    \"\"\"Performs Newton's optimization method with a simple line search.\n",
        "\n",
        "    Args:\n",
        "        f: function that when evaluated on a Theta of same dtype and shape as Theta0\n",
        "            returns a tuple (value, gradient, hessian), where gradient and Hessian\n",
        "            have the same shape as Theta.\n",
        "        theta0: starting point.\n",
        "        alpha: step length for backtracking line search (default is 1).\n",
        "        stop_tolerance: stop iterations when the norm of the gradient is below this threshold.\n",
        "        max_steps: maximum number of iterations.\n",
        "    Returns:\n",
        "        tuple:\n",
        "        - theta: optimal Theta after convergence or maximum steps.\n",
        "        - history: list of tuples (theta, value, gradient) containing the optimization path.\n",
        "    \"\"\"\n",
        "    theta = theta0\n",
        "    history = []\n",
        "    # TODO\n",
        "\n",
        "\n",
        "    return theta, history\n",
        "\n",
        "# Test Newton's method on the Rosenbrock function\n",
        "X0 = [0., 2.]  # Initial guess\n",
        "Xopt, Xhist = Newton(rosenbrock_hessian, X0)\n",
        "\n",
        "print(\"Found optimum at %s (true minimum is at [1,1])\" % Xopt)"
      ],
      "metadata": {
        "id": "JQ1Bf6sNMT_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part two: MLP for MNIST Classification\n",
        "In this part, we are going to use `PyTorch`. If you want to become more familiar with it, check this resource: https://www.learnpytorch.io/\n",
        "\n",
        "#### In this homework, you need to\n",
        "- implement SGD optimizer (`./optimizer.py`)\n",
        "- implement forward and backward for FCLayer (`layers.py`)\n",
        "- implement forward and backward for SigmoidLayer (`layers.py`)\n",
        "- implement forward and backward for ReLULayer (`layers.py`)\n",
        "- implement forward and backward for DropoutLayer (`layers.py`)\n",
        "- implement train and test process (`solver.py`)"
      ],
      "metadata": {
        "id": "wNXtvq8u-JSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from layers import FCLayer, SigmoidLayer, ReLULayer\n",
        "from solver import train, test\n",
        "from optimizer import SGD"
      ],
      "metadata": {
        "id": "zWtm2k5Ir-XQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "NRwL2rM0O92Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset\n",
        "\n",
        "# Converts PIL image to tensor and scales to [0, 1] and flatten it\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x.view(-1)),\n",
        "])\n",
        "\n",
        "# Load MNIST dataset with the defined transform\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# One-hot encoding for labels\n",
        "def decode_label(label, num_classes=10):\n",
        "    return torch.nn.functional.one_hot(torch.tensor(label), num_classes=num_classes).float()\n",
        "\n",
        "# Preprocess labels and combine with transformed images\n",
        "y_train = [decode_label(label) for _, label in train_dataset]\n",
        "y_test = [decode_label(label) for _, label in test_dataset]\n",
        "\n",
        "# Convert the data into tensor datasets for training and testing\n",
        "train_dataset = TensorDataset(torch.stack([img for img, _ in train_dataset]), torch.stack(y_train))\n",
        "test_dataset = TensorDataset(torch.stack([img for img, _ in test_dataset]), torch.stack(y_test))\n",
        "\n",
        "# Create data loaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "bCXcnjSYaR-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epoch = 20\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Build MLP with FCLayer and SigmoidLayer you've implemented in layers.py\n",
        "sigmoidMLP = nn.Sequential(\n",
        "    FCLayer(784, 128),\n",
        "    SigmoidLayer(),\n",
        "    FCLayer(128, 10)\n",
        ")\n",
        "\n",
        "# Initialize optimizer you've implemented in optimizer.py\n",
        "sgd = SGD(params=sigmoidMLP.parameters(), learning_rate=0.01)\n",
        "\n",
        "# Train the model using train function you've implemented in solver.py\n",
        "sigmoidMLP = train(sigmoidMLP, criterion, sgd, train_dataloader, num_epoch, device=device)"
      ],
      "metadata": {
        "id": "NkRty2cM-kIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your model using test function you've implemented in solver.py\n",
        "test(sigmoidMLP, test_dataloader, device)"
      ],
      "metadata": {
        "id": "F0otZUFrr7WW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build MLP with FCLayer and ReLULayer\n",
        "reluMLP = nn.Sequential(\n",
        "    FCLayer(784, 128),\n",
        "    ReLULayer(),\n",
        "    FCLayer(128, 10)\n",
        ")\n",
        "\n",
        "# Initialize optimizer\n",
        "sgd = SGD(reluMLP.parameters(), learning_rate=0.01)\n",
        "\n",
        "# Train the model\n",
        "reluMLP = train(reluMLP, criterion, sgd, train_dataloader, num_epoch, device=device)"
      ],
      "metadata": {
        "id": "n-2V-B_er7T2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "test(reluMLP, test_dataloader, device)"
      ],
      "metadata": {
        "id": "sLjxTbq8r7RZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overfit the model\n",
        "Try to overfit the reluMLP model. You can make the model as complex as you like, use subset of the data for training or any other approach you want.\n",
        "Then add **DropoutLayer** to your model in order to reduce overfitting problem."
      ],
      "metadata": {
        "id": "krsqZLJ-qQKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: overfit the reluMLP model"
      ],
      "metadata": {
        "id": "XbVFtro9qP1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from layers import DropoutLayer\n",
        "\n",
        "#TODO: add DropoutLayer to your model"
      ],
      "metadata": {
        "id": "BeCfvh6Jr7Om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UShs-KYlr7L4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0rg3t81mqNm9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}